"""
Production-Ready Retail ETL Pipeline DAG
=======================================
This DAG orchestrates an ETL pipeline for retail customer analytics using Amazon EMR.
It includes robust error handling, retry logic, and secure data processing.

I'll analyze the provided Airflow DAG file for a retail ETL pipeline. This appears to be a production-ready data pipeline for retail customer analytics using Amazon EMR.

## Overview of the DAG

This is a well-structured Airflow DAG that handles an ETL (Extract, Transform, Load) process for retail data using Amazon EMR (Elastic MapReduce). The pipeline follows best practices for production environments including:

1. Robust error handling
2. Retry logic
3. Logging and monitoring
4. Security considerations
5. Notification systems
6. Task orchestration

Let me break down the key components and flow of this pipeline:

### Key Components

1. **Error Handling System**:
   - Custom failure alert functions
   - CloudWatch logging
   - Conditional retry logic based on exception types
   - Specialized EMR failure handling

2. **Monitoring**:
   - Metrics emission to CloudWatch
   - Task duration tracking
   - Health checks for dependent services

3. **Security Features**:
   - EBS encryption for data at rest
   - SSL enabled for data in transit
   - Server-side encryption for S3 uploads

4. **Configuration Management**:
   - Dynamic loading of EMR configurations
   - Environment variable support
   - Tagging for resource management

5. **Data Quality Validation**:
   - File presence checks
   - Size validation
   - Placeholders for schema validation

### Pipeline Flow

The DAG defines a six-step workflow:

1. **Health Check**: Verifies connectivity to required AWS services
2. **Create EMR Cluster**: Provisions a new EMR cluster with security settings
3. **Submit PySpark ETL Job**: Adds a Spark job to process retail data
4. **Monitor EMR Step**: Waits for the ETL job to complete
5. **Data Quality Check**: Validates the processed data
6. **Terminate EMR Cluster**: Cleans up resources when finished
7. **Send Success Email**: Notifies stakeholders of successful completion

### Best Practices Implemented

- **Idempotency**: The DAG uses execution dates to ensure runs don't conflict
- **Resource Management**: Terminates clusters even if steps fail (trigger_rule='all_done')
- **Reliability**: Includes retry mechanisms with exponential backoff
- **Performance**: Configures Spark with memory settings and speculation for slow tasks
- **Monitoring**: Emits metrics for observability and duration tracking
- **Security**: Includes encryption for data at rest and in transit
- **Alerting**: Sends notifications through SNS and email on failures

This is a mature, production-grade implementation that follows many cloud and data engineering best practices for reliable, secure data processing at scale.

Would you like me to elaborate on any specific aspect of this DAG? For example, I could dive deeper into the error handling system, the EMR configuration management, or suggest any potential improvements.
"""

from airflow import DAG
from airflow.providers.amazon.aws.operators.emr import (
    EmrCreateJobFlowOperator, 
    EmrAddStepsOperator, 
    EmrTerminateJobFlowOperator
)
from airflow.providers.amazon.aws.sensors.emr import EmrStepSensor
from airflow.operators.python import PythonOperator
from airflow.operators.email import EmailOperator
from airflow.utils.dates import days_ago
from airflow.models import Variable
from airflow.utils.email import send_email
from airflow.hooks.base_hook import BaseHook
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from datetime import datetime, timedelta
import boto3
import json
import logging
import os
from typing import Any, Dict, Optional, Union, Type
from socket import timeout as TimeoutError
from requests.exceptions import ConnectionError
import time

# Configure logging
logger = logging.getLogger(__name__)


# ====== Constants and Environment Variables ====== #

# Load configurations from Airflow variables or environment variables
EMR_CONFIG_FILE = Variable.get("EMR_CONFIG_FILE", "/opt/airflow/dags/configs/emr_cluster_config.json")
BUCKET_NAME = Variable.get("S3_BUCKET_NAME", "your-bucket")
SNS_TOPIC_ARN = Variable.get("SNS_ALERT_TOPIC_ARN", "arn:aws:sns:us-east-1:123456789012:airflow-alerts")
NOTIFICATION_EMAIL = Variable.get("NOTIFICATION_EMAIL", "your-team@example.com")
LOG_GROUP_NAME = Variable.get("LOG_GROUP_NAME", "/aws/airflow/retail-etl")
AWS_REGION = Variable.get("AWS_REGION", "us-east-1")
SOURCE_PATH = f"s3://{BUCKET_NAME}/raw_data/"
DESTINATION_PATH = f"s3://{BUCKET_NAME}/processed_data/"
SPARK_JOB_PATH = f"s3://{BUCKET_NAME}/emr_jobs/retail_etl_job.py"


# ====== Error Handling & Notification Functions ====== #

def task_failure_alert(context):
    """
    Custom failure alert function for Airflow tasks.
    Logs detailed error and sends notifications via SNS and email.
    """
    # Extract context information
    dag_id = context.get('dag').dag_id
    task_id = context.get('task_instance').task_id
    execution_date = context.get('execution_date')
    log_url = context.get('task_instance').log_url
    error = context.get('exception')
    
    # Format the failure message
    message = (f"‚ùå Airflow Task Failure Detected! \n\n"
               f"DAG: {dag_id}\n"
               f"Task: {task_id}\n"
               f"Execution Time: {execution_date}\n"
               f"Log URL: {log_url}\n"
               f"Error: {error}\n")
    
    # Log the detailed error
    logger.error(f"Task Failed! DAG: {dag_id}, Task: {task_id}, Error: {error}")
    
    # Send SNS notification
    try:
        session = boto3.Session(region_name=AWS_REGION)
        sns_client = session.client('sns')
        
        response = sns_client.publish(
            TopicArn=SNS_TOPIC_ARN,
            Subject=f"Airflow Task Failed: {dag_id} - {task_id}",
            Message=message
        )
        logger.info(f"SNS notification sent! Message ID: {response['MessageId']}")
    except Exception as sns_error:
        logger.error(f"Failed to send SNS notification: {sns_error}")
    
    # Send email to support team
    try:
        send_email(
            to=NOTIFICATION_EMAIL,
            subject=f"Task Failure: {dag_id} - {task_id}",
            html_content=f"Task {task_id} failed in DAG {dag_id} at {execution_date}.<br>Error: {error}<br>Log URL: {log_url}"
        )
    except Exception as email_error:
        logger.error(f"Failed to send email notification: {email_error}")

    # Log metrics for monitoring
    emit_metric('task_failure', {'dag_id': dag_id, 'task_id': task_id}, 1)


def log_to_cloudwatch(message: str) -> None:
    """
    Logs messages to AWS CloudWatch for external monitoring.
    """
    try:
        session = boto3.Session(region_name=AWS_REGION)
        client = session.client('logs')
        log_stream_name = f"task-errors-{datetime.now().strftime('%Y-%m-%d')}"
        
        # Create log stream if it doesn't exist
        try:
            client.create_log_stream(
                logGroupName=LOG_GROUP_NAME,
                logStreamName=log_stream_name
            )
        except client.exceptions.ResourceAlreadyExistsException:
            pass  # Stream already exists, which is fine
        
        # Put log events to CloudWatch
        client.put_log_events(
            logGroupName=LOG_GROUP_NAME,
            logStreamName=log_stream_name,
            logEvents=[
                {
                    'timestamp': int(datetime.now().timestamp() * 1000),
                    'message': message
                }
            ]
        )
    except Exception as e:
        logger.error(f"Failed to log to CloudWatch: {str(e)}")


def retry_conditionally(context):
    """
    Implements conditional retry logic based on the exception type.
    Returns True to trigger a retry, False otherwise.
    """
    exception = context.get('exception')
    task_id = context.get('task_instance').task_id
    
    # Define which exceptions should trigger a retry
    if isinstance(exception, ConnectionError):
        logger.info(f"Retrying task {task_id} due to network error...")
        return True
    elif isinstance(exception, TimeoutError):
        logger.info(f"Retrying task {task_id} due to timeout...")
        return True
    elif isinstance(exception, boto3.exceptions.Boto3Error):
        logger.info(f"Retrying task {task_id} due to AWS API error: {exception}")
        return True
    elif "ServiceUnavailable" in str(exception):  # AWS service unavailable
        logger.info(f"Retrying task {task_id} due to AWS service unavailability...")
        return True
    else:
        logger.error(f"Non-retryable error encountered in task {task_id}: {exception}")
        # Log to CloudWatch for serious errors
        log_to_cloudwatch(f"Non-retryable error in task {task_id}: {exception}")
        return False


def emr_step_failure_alert(context):
    """
    Specific handler for EMR step failures with custom logic based on exception types.
    """
    exception = context.get('exception')
    task_id = context.get('task_instance').task_id
    
    # Handle specific EMR-related errors
    if isinstance(exception, ValueError):
        logger.error(f"EMR step {task_id} failed due to invalid configuration: {exception}")
    elif isinstance(exception, TimeoutError):
        logger.error(f"EMR step {task_id} failed due to timeout: {exception}")
    elif "ClusterTerminated" in str(exception):
        logger.error(f"EMR step {task_id} failed because cluster was terminated: {exception}")
    else:
        logger.error(f"EMR step {task_id} failed due to unknown error: {exception}")
    
    # Forward to the general failure handler
    task_failure_alert(context)
    
    # Record specific EMR failure metric
    emit_metric('emr_step_failure', {'task_id': task_id}, 1)


# ====== Metrics and Monitoring Functions ====== #

def emit_metric(metric_name: str, dimensions: Dict[str, str], value: float = 1) -> None:
    """
    Emits a CloudWatch metric for monitoring.
    """
    try:
        session = boto3.Session(region_name=AWS_REGION)
        cw_client = session.client('cloudwatch')
        
        dimension_list = [{'Name': k, 'Value': v} for k, v in dimensions.items()]
        
        cw_client.put_metric_data(
            Namespace='RetailETL',
            MetricData=[
                {
                    'MetricName': metric_name,
                    'Dimensions': dimension_list,
                    'Value': value,
                    'Unit': 'Count'
                }
            ]
        )
        logger.debug(f"Emitted metric {metric_name} with value {value}")
    except Exception as e:
        logger.error(f"Failed to emit metric {metric_name}: {e}")


def record_task_duration(context):
    """
    Records task duration as a CloudWatch metric.
    """
    task_instance = context.get('task_instance')
    task_id = task_instance.task_id
    
    start_time = task_instance.start_date
    end_time = task_instance.end_date
    
    if start_time and end_time:
        duration = (end_time - start_time).total_seconds()
        emit_metric('task_duration', {'task_id': task_id}, duration)


# ====== Helper Functions ====== #

def load_emr_config():
    """
    Loads EMR cluster configuration from a JSON file or Airflow Variable.
    """
    try:
        # Try to load from an Airflow Variable first
        config_str = Variable.get("EMR_CLUSTER_CONFIG", None)
        if config_str:
            config = json.loads(config_str)
        else:
            # Fall back to file
            with open(EMR_CONFIG_FILE, 'r') as f:
                config = json.load(f)
            
        # Add environment-specific tags
        env = Variable.get("ENVIRONMENT", "production")
        if 'Tags' not in config:
            config['Tags'] = []
            
        config['Tags'].append({'Key': 'Environment', 'Value': env})
        config['Tags'].append({'Key': 'ManagedBy', 'Value': 'Airflow'})
        config['Tags'].append({'Key': 'DAG', 'Value': 'retail_etl_pipeline'})
        
        # Ensure EBS encryption is enabled for data at rest
        if 'Instances' in config:
            if 'EbsConfiguration' not in config['Instances']:
                config['Instances']['EbsConfiguration'] = {
                    'EbsOptimized': True,
                    'EbsBlockDeviceConfigs': [
                        {
                            'VolumeSpecification': {
                                'SizeInGB': 100,
                                'VolumeType': 'gp2',
                                'Encrypted': True,
                            },
                            'VolumesPerInstance': 1,
                        }
                    ],
                }
            
        # Ensure SSL is enabled for data in transit
        if 'Configurations' not in config:
            config['Configurations'] = []
            
        found_core_site = False
        for conf in config['Configurations']:
            if conf.get('Classification') == 'core-site':
                conf['Properties']['hadoop.ssl.enabled'] = 'true'
                found_core_site = True
                break
                
        if not found_core_site:
            config['Configurations'].append({
                'Classification': 'core-site',
                'Properties': {
                    'hadoop.ssl.enabled': 'true',
                },
            })
            
        # Log config that was loaded (but remove sensitive information)
        safe_config = config.copy()
        if 'Ec2KeyName' in safe_config:
            safe_config['Ec2KeyName'] = '***REDACTED***'
        logger.info(f"Loaded EMR configuration: {json.dumps(safe_config)}")
        
        return config
    except Exception as e:
        logger.error(f"Failed to load EMR configuration: {e}")
        # Log to CloudWatch as this is a critical error
        log_to_cloudwatch(f"Failed to load EMR configuration: {e}")
        raise


def validate_data_quality(**kwargs):
    """
    Validates data quality by checking for the presence of processed files in S3.
    Performs additional validation on schema and data completeness.
    """
    try:
        # Use Airflow's S3Hook for better credential management
        s3_hook = S3Hook(aws_conn_id='aws_default')
        prefix = 'processed_data/'
        
        # List objects in the destination bucket
        keys = s3_hook.list_keys(bucket_name=BUCKET_NAME, prefix=prefix)
        
        # Check if files exist
        if not keys:
            raise ValueError(f"Data quality check failed: No files found in {prefix}")
            
        file_count = len(keys)
        logger.info(f"Data quality check: Found {file_count} files.")
        
        # Emit metric for file count
        emit_metric('processed_file_count', {}, file_count)
        
        # Additional validation: check total size of processed data
        total_size = 0
        for key in keys:
            obj = s3_hook.get_key(key, BUCKET_NAME)
            total_size += obj.size
        
        logger.info(f"Total size of processed data: {total_size/1024/1024:.2f} MB")
        emit_metric('processed_data_size_mb', {}, total_size/1024/1024)
        
        # Sample-based validation (for demo purposes - would be more comprehensive in production)
        # In real production, you might call a separate validation service or
        # run a validation Spark job to check schema compliance, data types, etc.
        
        # Optional: Check schema of a sample file if needed
        # This is just a placeholder for where you would implement schema validation
        
        # Report success
        logger.info("Data quality validation completed successfully")
        return True
            
    except Exception as e:
        logger.error(f"Data quality validation failed: {e}")
        # Log to CloudWatch
        log_to_cloudwatch(f"Data quality validation failed: {e}")
        raise


def upload_to_s3_with_encryption(bucket_name: str, file_path: str, s3_key: str) -> None:
    """
    Uploads a file to S3 with server-side encryption enabled.
    """
    try:
        s3_hook = S3Hook(aws_conn_id='aws_default')
        
        # Upload with AES256 encryption
        s3_hook.load_file(
            file_path, 
            s3_key,
            bucket_name=bucket_name,
            encrypt=True,
            replace=True
        )
        logger.info(f"Successfully uploaded {file_path} to s3://{bucket_name}/{s3_key} with encryption")
    except Exception as e:
        logger.error(f"Failed to upload file to S3: {e}")
        log_to_cloudwatch(f"Failed to upload file to S3: {e}")
        raise


def healthy_check(**kwargs):
    """
    Performs a health check and records metrics for monitoring.
    """
    ts = time.time()
    logger.info(f"Running health check at {datetime.fromtimestamp(ts)}")
    
    # Record the DAG run 
    emit_metric('dag_execution', {'dag_id': 'retail_etl_pipeline'}, 1)
    
    # Check AWS services health that this DAG depends on
    try:
        # Check S3 connectivity
        s3_hook = S3Hook(aws_conn_id='aws_default')
        s3_hook.check_for_bucket(BUCKET_NAME)
        emit_metric('s3_health_check', {'bucket': BUCKET_NAME}, 1)
        
        # Check EMR connectivity
        session = boto3.Session(region_name=AWS_REGION)
        emr_client = session.client('emr')
        emr_client.list_clusters(ClusterStates=['WAITING', 'RUNNING'])
        emit_metric('emr_health_check', {}, 1)
        
        logger.info("Health check passed")
        return True
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        emit_metric('health_check_failure', {}, 1)
        # Don't raise the exception - log it but let the DAG continue
        return False


# ====== DAG Definition ====== #

# Default arguments for all tasks
default_args = {
    'owner': 'retail-team',
    'depends_on_past': False,
    'email': [NOTIFICATION_EMAIL],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'retry_exponential_backoff': True,
    'max_retry_delay': timedelta(minutes=60),
    'on_failure_callback': task_failure_alert,
    'on_success_callback': record_task_duration,
}

# DAG definition
with DAG(
    'retail_etl_pipeline',
    default_args=default_args,
    description='ETL pipeline for Retail customer analytics with robust error handling',
    schedule_interval='@daily',
    start_date=days_ago(1),
    catchup=False,
    tags=['retail', 'aws', 'emr', 's3', 'airflow'],
    max_active_runs=1,  # Prevent concurrent runs
) as dag:

    # Step 0: Health Check - verify connectivity to required services
    health_check = PythonOperator(
        task_id='health_check',
        python_callable=healthy_check,
        provide_context=True,
    )

    # Step 1: Create EMR Cluster
    create_emr_cluster = EmrCreateJobFlowOperator(
        task_id='create_emr_cluster',
        job_flow_overrides=load_emr_config(),
        aws_conn_id='aws_default',
        region_name=AWS_REGION,
        on_failure_callback=task_failure_alert,
        on_retry_callback=retry_conditionally,
    )

    # Step 2: Submit PySpark ETL Job
    add_steps = EmrAddStepsOperator(
        task_id='add_emr_steps',
        job_flow_id="{{ task_instance.xcom_pull(task_ids='create_emr_cluster', key='return_value') }}",
        aws_conn_id='aws_default',
        steps=[
            {
                'Name': 'retail_etl_job',
                'ActionOnFailure': 'CONTINUE',
                'HadoopJarStep': {
                    'Jar': 'command-runner.jar',
                    'Args': [
                        'spark-submit',
                        '--deploy-mode', 'cluster',
                        '--conf', 'spark.yarn.maxAppAttempts=3',  # Allow Spark job retry
                        '--conf', 'spark.speculation=true',  # Enable speculation for slow tasks
                        '--conf', 'spark.driver.memory=4g',
                        '--conf', 'spark.executor.memory=8g',
                        SPARK_JOB_PATH,
                        '--source_path', SOURCE_PATH,
                        '--destination_path', DESTINATION_PATH,
                        '--date', "{{ ds }}"  # Automatically use execution date
                    ],
                },
            }
        ],
        on_failure_callback=emr_step_failure_alert,
        on_retry_callback=retry_conditionally,
    )

    # Step 3: Monitor EMR Step Completion
    monitor_emr_step = EmrStepSensor(
        task_id='monitor_emr_step',
        job_flow_id="{{ task_instance.xcom_pull('create_emr_cluster', key='return_value') }}",
        step_id="{{ task_instance.xcom_pull(task_ids='add_emr_steps', key='return_value')[0] }}",
        aws_conn_id='aws_default',
        poke_interval=60,  # Check every minute
        timeout=7200,  # 2 hours timeout
        on_failure_callback=task_failure_alert,
        on_retry_callback=retry_conditionally,
    )

    # Step 4: Data Quality Check
    data_quality_check = PythonOperator(
        task_id='data_quality_check',
        python_callable=validate_data_quality,
        provide_context=True,
        on_failure_callback=task_failure_alert,
        on_retry_callback=retry_conditionally,
    )

    # Step 5: Terminate EMR Cluster
    terminate_emr_cluster = EmrTerminateJobFlowOperator(
        task_id='terminate_emr_cluster',
        job_flow_id="{{ task_instance.xcom_pull(task_ids='create_emr_cluster', key='return_value') }}",
        aws_conn_id='aws_default',
        trigger_rule='all_done',  # Terminate even if steps fail
        on_failure_callback=task_failure_alert,
    )

    # Step 6: Send Success Email
    send_success_email = EmailOperator(
        task_id='send_success_email',
        to=NOTIFICATION_EMAIL,
        subject='Retail ETL Pipeline Succeeded: {{ ds }}',
        html_content="""
        <h3>ETL Pipeline Completed Successfully</h3>
        <p>The Retail ETL pipeline has completed all tasks successfully for date: {{ ds }}.</p>
        <p>You can access the processed data at: <code>s3://{{ params.bucket }}/processed_data/{{ ds }}/</code></p>
        """,
        params={'bucket': BUCKET_NAME},
        trigger_rule='all_success',
    )

    # Define the workflow
    health_check >> create_emr_cluster >> add_steps >> monitor_emr_step >> data_quality_check >> terminate_emr_cluster >> send_success_email